# -*- coding: utf-8 -*-
"""F30k_PreTrainGCVIT_ATT_LSTM_Latest_Ch2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eWIB4Q19pCrIfb5urVK2YmetWjmWVfiN
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/flickr30k_images.zip -d all_images
!unzip /content/drive/MyDrive/Flickr30k_text.zip -d all_captions

!pip install tensorflow==2.15.0
# !pip install keras==3.2.0
# !pip install keras_cv==0.7.2
!pip install gtts
!pip install Keras-Preprocessing
!pip install tensorflow_addons

# !pip install --quiet vit-keras
# !pip install "git+https://github.com/salaniz/pycocoevalcap.git"
# !pip install pycocoevalcap

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import tensorflow as tf
import keras
from keras_preprocessing.image import load_img
import string
import time
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.models import Model
from keras import layers
from keras import activations
from keras import Input
#from keras.utils.vis_utils import plot_model
from PIL import Image
from tqdm import tqdm
import glob
from gtts import gTTS
#from playsound import playsound
from IPython import display
import collections
import wordcloud
from wordcloud import WordCloud, STOPWORDS
import numpy as np
import pandas as pd
import os
import random
import statistics
from math import sqrt
import imageio
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import re
import numpy as np
import os
import collections
import random
import numpy as np
import os
import time
import json
import glob
from glob import glob
from PIL import Image
import pickle
# from vit_keras import vit
import warnings
import random
warnings.filterwarnings("ignore")

import os
import shutil

# Path to the directory containing all the images
image_directory = '/content/all_images/flickr30k_images/flickr30k_images'

# Path to the file containing all the captions
caption_file = '/content/all_captions/Flickr30k_text/Flickr30k_text/captions.txt'

# Path to the text file containing the training image IDs
training_ids_file = '/content/all_captions/Flickr30k_text/Flickr30k_text/train.txt'

# Create the output folder if it doesn't exist
output_folder = 'all_data'
os.makedirs(output_folder, exist_ok=True)

# Path to the output image folder
output_image_folder = os.path.join(output_folder, 'training_images')
os.makedirs(output_image_folder, exist_ok=True)

# Path to the output text file
output_text_file = os.path.join(output_folder, 'training_captions.txt')

# Read the training image IDs from the text file
with open(training_ids_file, 'r') as f:
    training_ids = f.read().splitlines()


TRAIN_LIMIT = 6000    #29000

training_ids = training_ids[:TRAIN_LIMIT] #for limit of train ids


# Create a dictionary to store the captions for training images
captions = {}

# Read the captions from the caption file
with open(caption_file, 'r') as f:
    for line in f:
        line = line.strip()
        if line:
            image_id, caption = line.split(',', 1)
            image_id = image_id.split('.')[0]  # Remove the file extension for comparison
            if image_id in training_ids:
                if image_id not in captions:
                    captions[image_id] = []
                captions[image_id].append(caption.strip())

# Write the image IDs and captions to the output text file
with open(output_text_file, 'w') as f:
    for image_id in training_ids:
        # Form the image filename
        image_filename = f'{image_id}.jpg'

        # Copy the image to the output image folder
        src_path = os.path.join(image_directory, image_filename)
        dst_path = os.path.join(output_image_folder, image_filename)
        if os.path.exists(src_path):
            shutil.copyfile(src_path, dst_path)
        else:
            print(f"Warning: {image_filename} does not exist in {image_directory}")

        # Write the image ID and captions to the output text file
        if image_id in captions:
            caption_list = captions[image_id]
            for caption in caption_list:
                f.write(f"{image_filename},{caption}\n")
        else:
            f.write(f"{image_filename},\n")

# Path to the directory containing all the images
image_directory = '/content/all_images/flickr30k_images/flickr30k_images'

# Path to the file containing all the captions
caption_file = '/content/all_captions/Flickr30k_text/Flickr30k_text/captions.txt'

# Path to the text file containing the training image IDs
training_ids_file = '/content/all_captions/Flickr30k_text/Flickr30k_text/val.txt'

# Create the output folder if it doesn't exist
output_folder = 'all_data'
os.makedirs(output_folder, exist_ok=True)

# Path to the output image folder
output_image_folder = os.path.join(output_folder, 'validation_images')
os.makedirs(output_image_folder, exist_ok=True)

# Path to the output text file
output_text_file = os.path.join(output_folder, 'validation_captions.txt')

# Read the training image IDs from the text file
with open(training_ids_file, 'r') as f:
    training_ids = f.read().splitlines()

VALID_LIMIT = 1000

training_ids = training_ids[:VALID_LIMIT] #for limit of validation ids


# Create a dictionary to store the captions for training images
captions = {}

# Read the captions from the caption file
with open(caption_file, 'r') as f:
    for line in f:
        line = line.strip()
        if line:
            image_id, caption = line.split(',', 1)
            image_id = image_id.split('.')[0]  # Remove the file extension for comparison
            if image_id in training_ids:
                if image_id not in captions:
                    captions[image_id] = []
                captions[image_id].append(caption.strip())

# Write the image IDs and captions to the output text file
with open(output_text_file, 'w') as f:
    for image_id in training_ids:
        # Form the image filename
        image_filename = f'{image_id}.jpg'

        # Copy the image to the output image folder
        src_path = os.path.join(image_directory, image_filename)
        dst_path = os.path.join(output_image_folder, image_filename)
        if os.path.exists(src_path):
            shutil.copyfile(src_path, dst_path)
        else:
            print(f"Warning: {image_filename} does not exist in {image_directory}")

        # Write the image ID and captions to the output text file
        if image_id in captions:
            caption_list = captions[image_id]
            for caption in caption_list:
                f.write(f"{image_filename},{caption}\n")
        else:
            f.write(f"{image_filename},\n")

# Path to the directory containing all the images
image_directory = '/content/all_images/flickr30k_images/flickr30k_images'

# Path to the file containing all the captions
caption_file = '/content/all_captions/Flickr30k_text/Flickr30k_text/captions.txt'

# Path to the text file containing the training image IDs
training_ids_file = '/content/all_captions/Flickr30k_text/Flickr30k_text/test.txt'

# Create the output folder if it doesn't exist
output_folder = 'all_data'
os.makedirs(output_folder, exist_ok=True)

# Path to the output image folder
output_image_folder = os.path.join(output_folder, 'testing_images')
os.makedirs(output_image_folder, exist_ok=True)

# Path to the output text file
output_text_file = os.path.join(output_folder, 'testing_captions.txt')

# Read the training image IDs from the text file
with open(training_ids_file, 'r') as f:
    training_ids = f.read().splitlines()

TEST_LIMIT=1000

training_ids = training_ids[:TEST_LIMIT] #for limit of Test ids


# Create a dictionary to store the captions for training images
captions = {}

# Read the captions from the caption file
with open(caption_file, 'r') as f:
    for line in f:
        line = line.strip()
        if line:
            image_id, caption = line.split(',', 1)
            image_id = image_id.split('.')[0]  # Remove the file extension for comparison
            if image_id in training_ids:
                if image_id not in captions:
                    captions[image_id] = []
                captions[image_id].append(caption.strip())

# Write the image IDs and captions to the output text file
with open(output_text_file, 'w') as f:
    for image_id in training_ids:
        # Form the image filename
        image_filename = f'{image_id}.jpg'

        # Copy the image to the output image folder
        src_path = os.path.join(image_directory, image_filename)
        dst_path = os.path.join(output_image_folder, image_filename)
        if os.path.exists(src_path):
            shutil.copyfile(src_path, dst_path)
        else:
            print(f"Warning: {image_filename} does not exist in {image_directory}")

        # Write the image ID and captions to the output text file
        if image_id in captions:
            caption_list = captions[image_id]
            for caption in caption_list:
                f.write(f"{image_filename},{caption}\n")
        else:
            f.write(f"{image_filename},\n")

# Paths
img_patht = '/content/all_data/training_images/'
caption_file = '/content/all_data/training_captions.txt'

# Initialize lists
all_idst = []
all_img_vectort = []
annotationst = []

# Read the captions from the caption file
with open(caption_file, 'r') as f:
    for line in f:
        line = line.strip()
        if line:
            image_idt, captiont = line.split(',', 1)
            full_flickr_image_patht = os.path.join(img_patht, image_idt)

            all_idst.append(image_idt)
            all_img_vectort.append(full_flickr_image_patht)
            annotationst.append(captiont)

# Shuffling the captions and image names together, setting a random state
#random_state = random.randint(1, 100)
#print("Random State is ", random_state)
img_idst, all_img_vectort, annotationst = shuffle(all_idst, all_img_vectort, annotationst)

# Selecting the first 40000 captions from the shuffled set
num_examples = len(all_idst)

img_idst = img_idst[:num_examples]
all_img_vectort = all_img_vectort[:num_examples]
annotationst = annotationst[:num_examples]

# Creating a DataFrame
dftrain = pd.DataFrame(list(zip(img_idst, all_img_vectort, annotationst)), columns=['ID', 'Path', 'Caption'])
dftrain

# Paths
img_pathv = '/content/all_data/validation_images/'
caption_file = '/content/all_data/validation_captions.txt'

# Initialize lists
all_idsv = []
all_img_vectorv = []
annotationsv = []

# Read the captions from the caption file
with open(caption_file, 'r') as f:
    for line in f:
        line = line.strip()
        if line:
            image_idv, captionv = line.split(',', 1)
            full_flickr_image_pathv = os.path.join(img_pathv, image_idv)

            all_idsv.append(image_idv)
            all_img_vectorv.append(full_flickr_image_pathv)
            annotationsv.append(captionv)

# Shuffling the captions and image names together, setting a random state
#random_state = random.randint(1, 100)
#print("Random State is ", random_state)
img_idsv, all_img_vectorv, annotationsv = shuffle(all_idsv, all_img_vectorv, annotationsv)

# Selecting the first 40000 captions from the shuffled set
num_examples = len(all_idsv)

img_idsv = img_idsv[:num_examples]
all_img_vectorv = all_img_vectorv[:num_examples]
annotationsv = annotationsv[:num_examples]

# Creating a DataFrame
dfval = pd.DataFrame(list(zip(img_idsv, all_img_vectorv, annotationsv)), columns=['ID', 'Path', 'Caption'])
dfval

# Paths
img_pathtst = '/content/all_data/testing_images/'
caption_file = '/content/all_data/testing_captions.txt'

# Initialize lists
all_idstst = []
all_img_vectortst = []
annotationstst = []

# Read the captions from the caption file
with open(caption_file, 'r') as f:
    for line in f:
        line = line.strip()
        if line:
            image_idtst, captiontst = line.split(',', 1)
            full_flickr_image_pathtst = os.path.join(img_pathtst, image_idtst)

            all_idstst.append(image_idtst)
            all_img_vectortst.append(full_flickr_image_pathtst)
            annotationstst.append(captiontst)

# Shuffling the captions and image names together, setting a random state
random_state = random.randint(1, 100)
print("Random State is ", random_state)
img_idstst, all_img_vectortst, annotationstst = shuffle(all_idstst, all_img_vectortst, annotationstst, random_state=random_state)

# Selecting the first 40000 captions from the shuffled set
num_examples = len(all_idstst)

img_idstst = img_idstst[:num_examples]
all_img_vectortst = all_img_vectortst[:num_examples]
annotationstst = annotationstst[:num_examples]

# Creating a DataFrame
dftrain = pd.DataFrame(list(zip(img_idstst, all_img_vectortst, annotationstst)), columns=['ID', 'Path', 'Caption'])
dftrain

print(len(all_img_vectort))
print(len(all_img_vectorv))
print(len(all_img_vectortst))

print(len(annotationst))
print(len(annotationsv))
print(len(annotationstst))

#check total captions and images present in dataset
annotationstotal=annotationst+annotationsv
print("Total captions present in the dataset: "+ str(len(annotationstotal)))
# print("Total images present in the dataset: " + str(len(all_imgstotal)))

#Create vocabulary & counter for the captions
vocabulary = [word.lower() for line in annotationstotal for word in line.split()]
val_count = Counter(vocabulary)
print(val_count)

img_ids=img_idst+img_idsv
all_img_vector=all_img_vectort+all_img_vectorv
annotations=annotationst+annotationsv
df = pd.DataFrame(list(zip(img_ids,all_img_vector,annotations)),columns =['ID','Path', 'Caption'])
#print(df)
df

#Visualise the top 20 occuring words in the captions
print('No of total words :',len(val_count))
for word, count in val_count.most_common(20):
  print(word, ": ", count)
lst = val_count.most_common(20)
most_common_words_df = pd.DataFrame(lst, columns = ['Word', 'Count'])
most_common_words_df.plot.bar(x='Word', y='Count', width=0.6, color='skyblue', figsize=(15, 10))
plt.title("Top 20 most frequent words before processing", fontsize = 18, color= 'navy')
plt.xlabel("Words", fontsize = 14, color= 'navy')
plt.ylabel("Count", fontsize = 14, color= 'navy')

def caption_with_img_plot(image_id, frame) :
  capt = ("\n" *2).join(frame[frame['ID'] == image_id].Caption.to_list())
  fig, ax = plt.subplots()
  ax.set_axis_off()
  idx = pd.DataFrame.ID.to_list().index(image_id)
  im =  Image.open(pd.DataFrame.Path.iloc[idx])
  w, h = im.size[0], im.size[-1]
  ax.imshow(im)
  ax.text(w+50, h, capt, fontsize = 10, color = 'navy')
#caption_with_img_plot(pd.DataFrame.ID.iloc[20000], df_merge)

#data cleaning
rem_punct = str.maketrans('', '', string.punctuation)
for r in range(len(annotationstotal)) :
  line = annotationstotal[r]
  line = line.split()

  # remove ambiguity
  for word in line:
    if (word!=word.lower()):
      line = [word.lower() for word in line]

  # remove punctuation from each caption and hanging letters
  line = [word.translate(rem_punct) for word in line]
  line = [word for word in line if len(word) > 0]

  # remove numeric values
  line = [word for word in line if word.isalpha()]

  annotationstotal[r] = ' '.join(line)

#add the <start> & <end> token to all those captions as well
annotations = ['<start>' + ' ' + line + ' ' + '<end>' for line in annotationstotal]

#Create a list which contains all the path to the images
all_img_path = all_img_vector
##list contatining captions for an image
annotations[0:5]

import math
res=0
word_counts ={}
threshold = 2
for line in annotationstotal:
  for word in line.split():
    word_counts[word] = word_counts.get(word, 0) + 1

print(word_counts)

for i in range(len(annotationstotal)):
  #print(type(len(annotations[i].split())))
  res = res + len(annotationstotal[i].split())
res=res-len(annotationstotal)*2
#print ("The number of words in string are : " + str(res))
vocabulary = [word for line in annotationstotal for word in line.split() if word_counts[word] >= threshold]
val_count = Counter(vocabulary)
unq = len(val_count)-2
print("No of total unique words in vocabulary after data cleaning excluding <start> and <end>:",unq)

# Creating tokenizer
top_word_cnt = unq
tokenizer = Tokenizer(num_words = top_word_cnt+1, filters= '!"#$%^&*()_+.,:;-?/~`{}[]|\=@ ',
                      lower = True, char_level = False,
                      oov_token = 'UNK')

# Creating word-to-index and index-to-word mappings.
tokenizer.fit_on_texts(annotations)
#transform each text into a sequence of integers
train_seqs = tokenizer.texts_to_sequences(annotations)

# Add PAD token for zero
tokenizer.word_index['PAD'] = 0
tokenizer.index_word[0] = 'PAD'
print(tokenizer.oov_token)
print(tokenizer.index_word[0])

# Creating a word count for our tokenizer to visualize the Top 20 occuring words after text processing

tokenizer_top_words = [word for line in annotationstotal for word in line.split() ]

#tokenizer_top_words_count
tokenizer_top_words_count = collections.Counter(tokenizer_top_words)

for word, count in tokenizer_top_words_count.most_common(20) :
  print(word, ": ", count)

tokens = tokenizer_top_words_count.most_common(20)
most_com_words_df = pd.DataFrame(tokens, columns = ['Word', 'Count'])

#plot 20 most common occuring words after processing

lst2=tokenizer_top_words_count.most_common(20)
most_common_words_df=pd.DataFrame(lst2, columns = ['Word','Count'])
most_common_words_df.plot.bar(x = 'Word', y= 'Count', width=0.6, color = 'skyblue', figsize = (15, 10))
plt.title('Top 20 most frequent words after processing', fontsize =18, color= 'navy')
plt.xlabel('Words', fontsize =14, color= 'navy')
plt.ylabel('Counts', fontsize =14, color= 'navy')

# Pad each vector to the max_length of the captions  store it to a vairable
train_seqs_len = [len(seq) for seq in train_seqs]
longest_sentence_length = max(train_seqs_len)
print(longest_sentence_length)
cap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding= 'post', maxlen = longest_sentence_length,
                                                          dtype='int32', value=0)
print("The shape of Caption vector is :" + str(cap_vector.shape))

# creating list to store preprocessed images and setting up the Image shape
preprocessed_image = []
IMAGE_SHAPE = (256, 256)
#checking image format
#tf.keras.backend.image_data_format()

# for img in all_imgs[0:5] :
#     img = tf.io.read_file(img, name=None)
#     img = tf.image.decode_jpeg(img, channels=0)
#     img = tf.image.resize(img, (256, 256))
#     img = vit.preprocess_inputs(img)
#     preprocessed_image.append(img)

# # checking first five images post preprocessing
# Display_Images = preprocessed_image[0:5]
# figure, axes = plt.subplots(1,5)
# figure.set_figwidth(20)
# for ax, image in zip(axes, Display_Images) :
#   print('Shape after resize : ', image.shape)
#   ax.imshow(image)
#   #ax.grid('off')

!pip install -U git+https://github.com/awsaf49/gcvit-tf              # If it prompts for Restart. Cancel it

def load_images(image_path) :
  img = tf.io.read_file(image_path, name = None)
  img = tf.image.decode_jpeg(img, channels=3)
  img = tf.image.resize(img, (224, 224))
  img = tf.keras.applications.imagenet_utils.preprocess_input(img, mode='tf')
  return img, image_path

# Map each image full path to the function, to preprocess the image
training_list = sorted(set(all_img_vector))
New_Img = tf.data.Dataset.from_tensor_slices(training_list)
New_Img = New_Img.map(load_images, num_parallel_calls = tf.data.experimental.AUTOTUNE)
New_Img = New_Img.batch(32, drop_remainder=False)



# path_train,  caption_train = all_img_vectort, cap_vector[:40000]        # 148915
# path_val, caption_val = all_img_vectorv, cap_vector[40000:45000]        # 148915:153915
# path_test, caption_test = all_img_vectortst, cap_vector[45000:]         # 153915:


path_train,  caption_train = all_img_vectort, cap_vector[:30000]
path_val, caption_val = all_img_vectorv, cap_vector[30000:]
#path_test, caption_test = all_img_vectortst, cap_vector[45000:]

print("Training data for images: " + str(len(path_train)))
print("Validation data for images: " + str(len(path_val)))
#print("Testing data for images: " + str(len(path_test)))
print("Training data for Captions: " + str(len(caption_train)))
print("Validation data for Captions: " + str(len(caption_val)))
#print("Testing data for Captions: " + str(len(caption_test)))
#print(path_test)

from gcvit import GCViTBase
IMAGE_SHAPE = (224, 224, 3)
image_features_extract_model = GCViTBase(
        input_shape = IMAGE_SHAPE,
        pretrain = True,
        )

# image_features_extract_model.reset_classifier(num_classes = 0, head_act = None)

# Freeze GCViT layers
for layer in image_features_extract_model.layers:
    layer.trainable = False
print(image_features_extract_model.summary())
# Get unique images

img_features={}
for img, path in tqdm(New_Img):
  batch_features = image_features_extract_model.forward_features(img)
  # Flatten the features (7,7,512) to (49, 512) for each image in the batch
  batch_features_flattened = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[-1]))

  for batch_feat, p in zip(batch_features_flattened, path):
    path_of_feature = p.numpy().decode("utf-8")
    np.save(path_of_feature, batch_feat.numpy())
    img_features[path_of_feature] = batch_feat.numpy()

# new_input =vit_model.input
# hidden_layer = vit_model.layers[-2].output
# image_features_extract_model = tf.keras.Model(new_input, hidden_layer)

# for layer in image_features_extract_model.layers:
#   layer.trainable = False
# #image_features_extract_model.summary()

# img_features = {}
# for image,image_path in tqdm(New_Img):
#   batch_features = image_features_extract_model(image)
#   #print("Batch Features Shape:", batch_features.shape)
#   #batch_features_flattened = tf.reshape(batch_features, (batch_features.shape[0], -1))
#   for batch_feat, path in zip(batch_features, image_path) :
#     feature_path = path.numpy().decode('utf-8')
#     img_features[feature_path] = batch_feat.numpy()

image_features_extract_model.summary()

batch_feat.shape

#func to provide, both images along with the captions as input
def map(image_name, caption):
    img_tensor = img_features[image_name.decode('utf-8')]
    return img_tensor, caption

#func to transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier
BUFFER_SIZE = 1000
BATCH_SIZE = 32
def gen_dataset(img, capt):

    data = tf.data.Dataset.from_tensor_slices((img, capt))
    data = data.map(lambda ele1, ele2 : tf.numpy_function(map, [ele1, ele2], [tf.float32, tf.int32]),
                    num_parallel_calls = tf.data.experimental.AUTOTUNE)


    data = (data.shuffle(BUFFER_SIZE, reshuffle_each_iteration= True).batch(BATCH_SIZE, drop_remainder = False)
    .prefetch(tf.data.experimental.AUTOTUNE))
    return data

train_dataset = gen_dataset(path_train,caption_train)
val_dataset = gen_dataset(path_val,caption_val)
#test_dataset = gen_dataset(path_test,caption_test)

sample_img_batch, sample_cap_batch = next(iter(train_dataset))
print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)
print(sample_cap_batch.shape) #(batch_size,max_len)


#Setting  parameters
embedding_dim = 512
units = 512

#top 60% words +1
vocab_size = top_word_cnt+1
train_num_steps = len(path_train) // BATCH_SIZE
val_num_steps = len(path_val) // BATCH_SIZE
#test_num_steps = len(path_test) // BATCH_SIZE

max_length = longest_sentence_length  #31
feature_shape = batch_feat.shape[1]
attention_feature_shape = batch_feat.shape[0]

class Encoder(Model):
    def __init__(self,embed_dim):
        super(Encoder, self).__init__()
        self.dense = tf.keras.layers.Dense(embed_dim)

    def call(self, features):
        features =  self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)
        # features =  tf.keras.activations.relu(features, alpha=0.01, max_value=None, threshold=0)
        features =  tf.keras.activations.relu(features)
        return features

class Attention(Model):
    def __init__(self, units):
        super(Attention, self).__init__()
        self.q = tf.keras.layers.Dense(units)
        self.k = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)
        self.units=units

    def call(self, features, hidden):
        hidden_with_time_axis = hidden[:, tf.newaxis]
        score = tf.keras.activations.tanh(self.q(features) + self.k(hidden_with_time_axis))
        attention_weights = tf.keras.activations.softmax(self.V(score), axis=1)
        context_vector = attention_weights * features
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

class Decoder(Model):
    def __init__(self, embed_dim, units, vocab_size):
        super(Decoder, self).__init__()
        self.units=units
        self.attention = Attention(self.units) #iniitalize Attention model by units
        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim) #building Embedding layer
        self.lstm = tf.keras.layers.LSTM(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')
        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer
        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer


    def call(self,x,features, hidden, cell):
        context_vector, attention_weights = self.attention(features, hidden) #create your context vector & attention weights from attention model
        embed = self.embed(x) # embed your input to shape: (batch_size, 1, embedding_dim)
        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis = -1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)
        output,hidden_state, cell_state = self.lstm(embed, initial_state=[hidden, cell]) # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)
        output = self.d1(output)
        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)
        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)

        return output, hidden_state, cell_state, attention_weights

    def init_state(self, batch_size):
        # Initialize both hidden and cell states
        return (tf.zeros((batch_size, self.units)),  # Hidden state
                tf.zeros((batch_size, self.units)))  # Cell state

encoder=Encoder(embedding_dim)
decoder=Decoder(embedding_dim, units, vocab_size)
features=encoder(sample_img_batch)
hidden_state, cell_state = decoder.init_state(batch_size=sample_cap_batch.shape[0])
dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)

predictions, hidden_out, cell_out, attention_weights= decoder(dec_input, features, hidden_state, cell_state)
print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)
print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch_size,vocab_size)
print('Attention weights shape: {}'.format(attention_weights.shape)) #(batch_size,257, embed_dim)

optimizer = tf.keras.optimizers.Nadam(learning_rate = 0.001)  #defining the optimizer
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE) #define your loss object
#loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE) #define your loss object


def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    return tf.reduce_mean(loss_)

checkpoint_path = "/content/checkpoint"
ckpt = tf.train.Checkpoint(encoder=encoder,
                           decoder=decoder,
                           optimizer = optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

start_epoch = 0
if ckpt_manager.latest_checkpoint:
    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])


@tf.function
def train_step(img_tensor, target):
    loss = 0
    hidden, cell = decoder.init_state(batch_size=target.shape[0])
    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)

    with tf.GradientTape() as tape:

        encoder_op = encoder(img_tensor)
        for r in range(1, target.shape[1]) :
          predictions, hidden, cell, _ = decoder(dec_input, encoder_op, hidden, cell)
          loss = loss + loss_function(target[:, r], predictions)
          dec_input = tf.expand_dims(target[:, r], 1)

    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch
    trainable_vars = encoder.trainable_variables + decoder.trainable_variables
    grad = tape.gradient (loss, trainable_vars)
    optimizer.apply_gradients(zip(grad, trainable_vars))

    return loss, avg_loss


@tf.function
def val_step(img_tensor, target):
    loss = 0
    all_predictions = []
    hidden, cell = decoder.init_state(batch_size = target.shape[0])
    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)
    with tf.GradientTape() as tape:
      encoder_op = encoder(img_tensor)
      for r in range(1, target.shape[1]) :
        predictions, hidden, cell, _ = decoder(dec_input, encoder_op, hidden, cell)
        all_predictions.append(predictions)                                         ######3
        loss = loss + loss_function(target[:, r], predictions)
        dec_input = tf.expand_dims(target[: , r], 1)

    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch
    trainable_vars = encoder.trainable_variables + decoder.trainable_variables
    grad = tape.gradient (loss, trainable_vars)
    optimizer.apply_gradients(zip(grad, trainable_vars))
    all_predictions = tf.stack(all_predictions, axis=1)                               ###(batch, seq len -1, vocab)
    return loss, avg_loss, predictions, all_predictions

# from collections import Counter, defaultdict
# from pycocoevalcap.bleu.bleu import Bleu
# from pycocoevalcap.spice.spice import Spice
# from pycocoevalcap.rouge.rouge import Rouge
# from pycocoevalcap.cider.cider import Cider
# from pycocoevalcap.meteor.meteor import Meteor
# import contextlib
# import io
# import hashlib
# import time
# import numpy as np
# import random

# f = io.StringIO()

# bleu = Bleu(n=4)
# meteor = Meteor()
# rouge = Rouge()
# cider = Cider()
# spice = Spice()

def decode_caption(caption):
    decoded_caption = []
    # print(type(caption))
    for word_id in caption:
        if word_id == 0:  # Padding token, ignore
            continue
        if word_id == tokenizer.word_index['<end>']:  # Stop decoding at <end> token
            break
        if word_id == tokenizer.word_index['<start>']:  # Ignore <start> token
            continue
        decoded_caption.append(tokenizer.index_word[word_id])
    return decoded_caption

def hash_image_tensor(img_tensor):
    """Hash image tensor to create a unique identifier for each image."""
    return hashlib.sha256(img_tensor.numpy().tobytes()).hexdigest()

def list_to_dict(input_list):
  return {i+1: [item] for i, item in enumerate(input_list)}

def take_first_three_elements(input_dict):
    return {key: value[:3] for key, value in input_dict.items()}

train_loss_plot = []
val_loss_plot = []
test_loss_plot = []
best_val_loss=100
best_test_loss=100
bleu1_scores = []
bleu2_scores = []
bleu3_scores = []
bleu4_scores = []
best_scores = {
    'BLEU-1': 0.0,
    'BLEU-2': 0.0,
    'BLEU-3': 0.0,
    'BLEU-4': 0.0,
    'METEOR': 0.0,
    'ROUGE-1': 0.0,
    'ROUGE-2': 0.0,
    'ROUGE-L': 0.0,
    'CIDEr' : 0.0,
    'SPICE' :0.0

}
bleu1_scores_plot = []
bleu2_scores_plot = []
bleu3_scores_plot = []
bleu4_scores_plot = []
meteor_scores_plot = []
rouge1_scores_plot = []
rouge2_scores_plot = []
rouge_l_scores_plot = []
cider_scores_plot = []
spice_scores_plot = []

patience = 3
wait = 0
early_stop = False

EPOCHS = 15

for epoch in tqdm(range(1, EPOCHS+1)):
    start = time.time()
    total_loss = 0
    total_loss_val=0
    image_ids = []
    references = []
    hypotheses = []

    references_dict = {}
    hypotheses_dict = {}

    for (batch, (img_tensor, tar)) in enumerate(train_dataset):
        batch_loss, t_loss = train_step(img_tensor, tar)
        total_loss += t_loss
        avg_train_loss = total_loss / train_num_steps

    for (batch, (img_tensor, tar)) in enumerate(val_dataset) :
        tar_real = tar[:, 1:]
        batch_loss, loss, predictions, all_predictions = val_step(img_tensor, tar)
        total_loss_val = total_loss_val + loss
        avg_val_loss = total_loss_val / val_num_steps

    #     for j in range(tar_real.shape[0]):
    #         img_id =  hash_image_tensor(img_tensor[j])
    #         reference = decode_caption(tf.boolean_mask(tar_real[j], tar_real[j] != 0).numpy())
    #         hypothesis = decode_caption(tf.argmax(all_predictions[j], axis = -1).numpy())
    #         references.append(" ".join(reference))  # Join the list of words into a single string
    #         hypotheses.append(" ".join(hypothesis))  # Join the list of words into a single string

    #         if img_id not in references_dict:
    #             references_dict[img_id] = []
    #         references_dict[img_id].append(" ".join(reference))
    #         if img_id not in hypotheses_dict:
    #             hypotheses_dict[img_id] = []
    #         hypotheses_dict[img_id].append(" ".join(hypothesis))

    # references = list_to_dict(references)
    # hypotheses = list_to_dict(hypotheses)

    # unique_references = list(references_dict.values())
    # unique_hypotheses = list(hypotheses_dict.values())

    # prep_references = {}
    # prep_hypotheses = {}
    # new_ref = {}
    # num_images = len(unique_references)

    # for i in range(num_images):
    #     img_id = str(i + 1)
    #     prep_references[img_id] = unique_references[i]########  prepare ref and hyp for multiple ref and single hyp
    #     prep_hypotheses[img_id] = unique_hypotheses[i][-1]
    #     # prep_hypotheses[img_id] = [unique_hypotheses[i][-1]]

    # prep_hypotheses = {image_id: [sentence] for image_id, sentence in prep_hypotheses.items()}
    # # new_ref = take_first_three_elements(prep_references)
    # with contextlib.redirect_stdout(f):
    #     bleu_scores, _ = bleu.compute_score(prep_references, prep_hypotheses)

    # rouge_score, _ = rouge.compute_score(prep_references, prep_hypotheses)

    # meteor_score, _ = meteor.compute_score(prep_references, prep_hypotheses)

    # cider_score, _ = cider.compute_score(prep_references, prep_hypotheses)

    # spice_score, _ = spice.compute_score(prep_references, prep_hypotheses)

    train_loss_plot.append(avg_train_loss)
    val_loss_plot.append(avg_val_loss)

    # best_scores['BLEU-1'] = max(best_scores['BLEU-1'], bleu_scores[0])
    # best_scores['BLEU-2'] = max(best_scores['BLEU-2'], bleu_scores[1])
    # best_scores['BLEU-3'] = max(best_scores['BLEU-3'], bleu_scores[2])
    # best_scores['BLEU-4'] = max(best_scores['BLEU-4'], bleu_scores[3])
    # best_scores['METEOR'] = max(best_scores['METEOR'], meteor_score)
    # best_scores['ROUGE-L'] = max(best_scores['ROUGE-L'], rouge_score)
    # best_scores['CIDEr'] = max(best_scores['CIDEr'], cider_score)
    # best_scores['SPICE'] = max(best_scores['SPICE'], spice_score)


    # bleu1_scores_plot.append(best_scores['BLEU-1'])
    # bleu2_scores_plot.append(best_scores['BLEU-2'])
    # bleu3_scores_plot.append(best_scores['BLEU-3'])
    # bleu4_scores_plot.append(best_scores['BLEU-4'])
    # meteor_scores_plot.append(best_scores['METEOR'])
    # rouge_l_scores_plot.append(best_scores['ROUGE-L'] )
    # cider_scores_plot.append(best_scores['CIDEr'])
    # spice_scores_plot.append(best_scores['SPICE'])

    # print(f'Epoch {epoch } - Evaluation scores:')
    print ('For epoch: {}, the train loss is {:.3f}, validation loss is {:.3f}'.format(epoch ,avg_train_loss, avg_val_loss))
    ## print ('BLEU-1: {:.3f}, BLEU-2: {:.3f}, BLEU-3: {:.3f}, BLEU-4: {:.3f}, ROUGE: {:.3f}, METEOR: {:.3f}, CIDEr: {:.3f}, SPICE: {:.3f} '.format(bleu_scores[0] ,bleu_scores[1], bleu_scores[2],bleu_scores[3],rouge_score,meteor_score,cider_score,spice_score))
    # print(f'BLEU-1: {best_scores["BLEU-1"]:.4f} BLEU-2: {best_scores["BLEU-2"]:.4f} BLEU-3: {best_scores["BLEU-3"]:.4f} BLEU-4: {best_scores["BLEU-4"]:.4f} METEOR: {best_scores["METEOR"]:.4f}  ROUGE-L: {best_scores["ROUGE-L"]:.4f} CIDEr: {best_scores["CIDEr"]:.4f} SPICE: {best_scores["SPICE"]:.4f}')
    print ('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

    if avg_val_loss < best_val_loss:
        # print('Validation loss has improved from {:.3f} to {:.3f}'.format(best_val_loss, avg_val_loss))
        best_val_loss = avg_val_loss
        wait = 0
        ckpt_manager.save()
    else:
        wait += 1
        # print(f'No improvement in validation loss. Current patience: {wait}/{patience}')
        if wait >= patience:
            print('===================')
            early_stop = True
            break

    if early_stop:
        break



# EPOCHS = 60

# for epoch in tqdm(range(31, EPOCHS+1)):
#     start = time.time()
#     total_loss = 0
#     total_loss_val=0
#     image_ids = []
#     references = []
#     hypotheses = []

#     references_dict = {}
#     hypotheses_dict = {}

#     for (batch, (img_tensor, tar)) in enumerate(train_dataset):
#         batch_loss, t_loss = train_step(img_tensor, tar)
#         total_loss += t_loss
#         avg_train_loss = total_loss / train_num_steps

#     for (batch, (img_tensor, tar)) in enumerate(val_dataset) :
#         tar_real = tar[:, 1:]
#         batch_loss, loss, predictions, all_predictions = val_step(img_tensor, tar)
#         total_loss_val = total_loss_val + loss
#         avg_val_loss = total_loss_val / val_num_steps

#         for j in range(tar_real.shape[0]):
#             img_id =  hash_image_tensor(img_tensor[j])
#             reference = decode_caption(tf.boolean_mask(tar_real[j], tar_real[j] != 0).numpy())
#             hypothesis = decode_caption(tf.argmax(all_predictions[j], axis = -1).numpy())
#             references.append(" ".join(reference))  # Join the list of words into a single string
#             hypotheses.append(" ".join(hypothesis))  # Join the list of words into a single string

#             if img_id not in references_dict:
#                 references_dict[img_id] = []
#             references_dict[img_id].append(" ".join(reference))
#             if img_id not in hypotheses_dict:
#                 hypotheses_dict[img_id] = []
#             hypotheses_dict[img_id].append(" ".join(hypothesis))

#     references = list_to_dict(references)
#     hypotheses = list_to_dict(hypotheses)

#     unique_references = list(references_dict.values())
#     unique_hypotheses = list(hypotheses_dict.values())

#     prep_references = {}
#     prep_hypotheses = {}
#     num_images = len(unique_references)

#     for i in range(num_images):
#         img_id = str(i + 1)
#         prep_references[img_id] = unique_references[i]########  prepare ref and hyp for multiple ref and single hyp
#         prep_hypotheses[img_id] = unique_hypotheses[i][-1]
#         # prep_hypotheses[img_id] = [unique_hypotheses[i][-1]]

#     prep_hypotheses = {image_id: [sentence] for image_id, sentence in prep_hypotheses.items()}

#     with contextlib.redirect_stdout(f):
#         bleu_scores, _ = bleu.compute_score(references, hypotheses)

#     rouge_score, _ = rouge.compute_score(references, hypotheses)

#     meteor_score, _ = meteor.compute_score(references, hypotheses)

#     cider_score, _ = cider.compute_score(prep_references, prep_hypotheses)

#     spice_score, _ = spice.compute_score(prep_references, prep_hypotheses)

#     train_loss_plot.append(avg_train_loss)
#     val_loss_plot.append(avg_val_loss)
#     bleu1_scores_plot.append(bleu_scores[0])
#     bleu2_scores_plot.append(bleu_scores[1])
#     bleu3_scores_plot.append(bleu_scores[2])
#     bleu4_scores_plot.append(bleu_scores[3])
#     meteor_scores_plot.append(meteor_score)
#     rouge_l_scores_plot.append(rouge_score)
#     cider_scores_plot.append(cider_score)
#     spice_scores_plot.append(spice_score)

#     print(f'Epoch {epoch } - Evaluation scores:')
#     print ('For epoch: {}, the train loss is {:.3f}, validation loss is {:.3f}'.format(epoch ,avg_train_loss, avg_val_loss))
#     # print ('BLEU-1: {:.3f}, BLEU-2: {:.3f}, BLEU-3: {:.3f}, BLEU-4: {:.3f}, ROUGE: {:.3f}, METEOR: {:.3f}, CIDEr: {:.3f}, SPICE: {:.3f} '.format(bleu_scores[0] ,bleu_scores[1], bleu_scores[2],bleu_scores[3],rouge_score,meteor_score,cider_score,spice_score))
#     print(f'BLEU-1: {best_scores["BLEU-1"]:.4f} BLEU-2: {best_scores["BLEU-2"]:.4f} BLEU-3: {best_scores["BLEU-3"]:.4f} BLEU-4: {best_scores["BLEU-4"]:.4f} METEOR: {best_scores["METEOR"]:.4f} ROUGE-1: {best_scores["ROUGE-1"]:.4f} ROUGE-2: {best_scores["ROUGE-2"]:.4f}  ROUGE-L: {best_scores["ROUGE-L"]:.4f} CIDEr: {best_scores["CIDEr"]:.4f} SPICE: {best_scores["SPICE"]:.4f}')
#     print ('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

#     if avg_val_loss < best_val_loss:
#         # print('Validation loss has improved from {:.3f} to {:.3f}'.format(best_val_loss, avg_val_loss))
#         best_val_loss = avg_val_loss
#         wait = 0
#         ckpt_manager.save()
#     else:
#         wait += 1
#         # print(f'No improvement in validation loss. Current patience: {wait}/{patience}')
#         if wait >= patience:
#             # print('Early stopping triggered!')
#             early_stop = True
#             break

#     if early_stop:
#         break

for i in range(len(train_loss_plot)):
    print("for epoch",i+1,"train loss is",train_loss_plot[i],"val loss is ",val_loss_plot[i])

# @title Default title text
from matplotlib.pyplot import figure
figure(figsize=(12, 8))
plt.plot(train_loss_plot, color='blue', label = 'Training Loss')
plt.plot(val_loss_plot, color='red', label ='Validation Loss')
#plt.plot(test_loss_plot, color='red', label = 'Test Loss')
plt.xlabel('Epochs', fontsize = 15, color = 'navy')
plt.ylabel('Loss', fontsize = 15, color = 'navy')
plt.title('Loss Plot', fontsize = 20, color = 'navy')
plt.legend()
plt.show()



"""## plots for 30 epochs"""

import openpyxl

def save_loss_to_excel(train_loss, val_loss, file_path):
    wb = openpyxl.Workbook()
    sheet = wb.active
    sheet.title = "Loss Data"

    # Write headers
    sheet["A1"] = "Iteration"
    sheet["B1"] = "Train Loss"
    sheet["C1"] = "Validation Loss"
    #sheet["D1"] = "Test Loss"
    # Write data
    for i, (train_loss_val, val_loss_val) in enumerate(zip(train_loss_plot, val_loss_plot), start=3):
        sheet[f"A{i}"] = i - 1  # Iteration number
        sheet[f"B{i}"] = train_loss_val.numpy()
        #sheet[f"B{i}"] = train_loss_val
        sheet[f"C{i}"] = val_loss_val.numpy()
        #sheet[f"D{i}"] = test_loss_val.numpy()

    # Save to Excel file
    wb.save(file_path)
    print(f"Loss data saved to {file_path}")

# Example usage
train_loss = train_loss_plot
val_loss = val_loss_plot
#test_loss = test_loss_plot
save_loss_to_excel(train_loss, val_loss, "lossdata_VitAttGRU26.xlsx")

# attention_feature_shape = batch_feat[0]#np.prod(attention_weights.shape)

def evaluate(image):
    # max_length = 20  # Define max length based on your model
    # attention_feature_shape = 257  # 256 patches + 1 class token
    height, width = 16, 16  # 16x16 spatial grid

    attention_plot = np.zeros((max_length, height, width))  # Initialize attention plot (word_length, height, width)

    hidden, cell = decoder.init_state(batch_size=1)

    temp_input = tf.expand_dims(load_images(image)[0], 0)  # Preprocess image
    img_tensor_val = image_features_extract_model.forward_features(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[-1]))

    features = encoder(img_tensor_val)

    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)
    result = []

    for i in range(max_length):
        predictions, hidden, cell, attention_weights = decoder(dec_input, features, hidden, cell)

        # Remove the class token attention weight and reshape the remaining 256 to (16, 16)
        # attention_weights = tf.reshape(attention_weights[:, 1:], (height, width))
        # attention_plot[i] = attention_weights.numpy()

        predicted_id = tf.argmax(predictions[0]).numpy()
        result.append(tokenizer.index_word[predicted_id])

        if tokenizer.index_word[predicted_id] == '<end>':
            # attention_plot = attention_plot[:len(result), :]  # Only keep attention up to the result length
            return result, predictions

        dec_input = tf.expand_dims([predicted_id], 0)

    # attention_plot = attention_plot[:len(result), :]  # Final attention plot
    return result, predictions




def filt_text(text):
    filt=['<start>','<unk>','<end>']
    temp= text.split()
    [temp.remove(j) for k in filt for j in temp if k==j]
    text=' '.join(temp)
    return text
#image_test = path_test.copy()
#image_val=path_val.copy()
image_test = path_val.copy()

import numpy as np
import matplotlib.pyplot as plt
import cv2
import urllib.request
import matplotlib.patches as patches
from matplotlib.patches import Rectangle

def add_color_box_label(fig, text, color, position, edgecolor='black'):
    ax = fig.add_axes(position)
    # Add a rectangle with a border
    rect = Rectangle((0, 0), 1, 1, facecolor=color, edgecolor=edgecolor, linewidth=1)
    ax.add_patch(rect)
    ax.axis('off')
    # Add the text label next to the colored box
    fig.text(position[0] + position[2] + 0.01, position[1] + position[3] / 2, text,
             va='center', ha='left', fontsize=9)

def show_attention_heatmap(image_path, attention, caption, save_image=False):
    # Load image from URL if needed
    if image_path.startswith('http'):
        image = np.asarray(bytearray(urllib.request.urlopen(image_path).read()), dtype="uint8")
        image = cv2.imdecode(image, cv2.IMREAD_COLOR)
    else:
        image = cv2.imread(image_path)

    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB

    num_words = len(caption)
    num_cols = 5  # Number of images per row
    num_rows = (num_words + num_cols - 1) // num_cols  # Compute number of rows

    # Set up plot grid with dynamic rows
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 3))

    # Flatten axes for easier indexing, if there's only one row it might not be a list
    if num_rows == 1:
        axes = axes.flatten()
    else:
        axes = axes.ravel()

    # Create a variable to store the attention maps for the color bar
    all_att_maps = []

    for i in range(num_words):
        word = caption[i]
        att_map = attention[i]

        # Resize the attention map to the image size
        att_map_resized = cv2.resize(att_map, (image.shape[1], image.shape[0]))

        # Normalize attention map between 0 and 1
        att_map_resized = att_map_resized / np.max(att_map_resized)

        # Store for global color bar scaling
        all_att_maps.append(att_map_resized)

        # Plot image
        axes[i].imshow(image, alpha=0.8)

        # Overlay attention map
        im = axes[i].imshow(att_map_resized, cmap='hot', alpha=0.5)  # Heatmap overlay
        axes[i].contour(att_map_resized, colors='yellow', linewidths=0.3)

        axes[i].set_title(f" {word}")
        axes[i].axis('off')

        if save_image:
            # Save each image with heatmap
            plt.imsave(f'heatmap_{word}.png', att_map_resized, cmap='jet')

    # Hide any extra subplots if num_words is not a multiple of num_cols
    for j in range(num_words, num_rows * num_cols):
        axes[j].axis('off')

    # Combine all attention maps for color bar normalization
    all_att_maps_combined = np.vstack(all_att_maps)

    # Add a single horizontal color bar for the entire plot at the top with increased height
    fig.subplots_adjust(top=0.78, bottom=0.1, right=0.95, left=0.05, hspace=0.3)  # Adjusted top for more space
    bar_width = 0.5  # Width of the color bar
    left_position = (1 - bar_width) / 2  # Center the color bar

    cbar_ax = fig.add_axes([left_position, 0.85, bar_width, 0.06])  # Centered color bar
    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')
    cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1])  # Ticks at normalized values
    cbar.set_ticklabels(['Low', '0.2', '0.4', '0.6', '0.8', 'High'])  # Custom labels
    fig.text(0.5, 0.98, 'Attention Focus Weight', ha='center', fontsize=12)
    add_color_box_label(fig, ' Focused Area', '#fdf98a', [0.8, 0.84, 0.02, 0.06])
    plt.show()

# Example usage
# image_url = '/content/all_data/validation_images/110802332.jpg'
# caption = ['A', 'man', 'on', 'a', 'horse']
# at

# image_url = '/content/all_data/validation_images/110802332.jpg'

# # Sample caption
# caption = ['A', 'man', 'on', 'a', 'horse']

# # Random attention maps for each word (shape: num_words x height x width)
# attention = np.random.rand(5,14,14)  # Example with (14x14) attention maps
# # Run the function with demo values
# show_attention_heatmap(image_url, attention, caption)



# print(type(attention))

def predicted_captiontest(image_name, autoplay=False) :
    autoplay = False
    print(image_name)
    print()

    # image_name = "/content/all_data/validation_images/110802332.jpg"

    rid = image_test.index(image_name)
    # print(rid)

    cap_test_data = caption_val.copy()
    # rid = 100
    # rid = random.sample(range(0, rndm),2)
    real_cap_list=[]
    pred_cap_list=[]
    j=rid
    test_image=image_test[j]
    #print(len(cap_test_data))
    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test_data[j] if i not in [0]])
    #print(real_caption)
    # result, attention_plot, pred_test = evaluate(test_image)
    result, pred_test = evaluate(test_image)
    # print(attention_plot.shape)

    real_caption=filt_text(real_caption)
    pred_caption=' '.join(result).rsplit(' ', 1)[0]
    real_appn = []
    real_appn.append(real_caption.split())
    real_cap_list.append(real_appn)
    pred_cap_list.append(pred_caption.split())

    #all_eval_method(test_image,real_cap_list, pred_cap_list)

    print("\033[1mReal Caption:\033[0m", real_caption)
    print("\033[1mPrediction Caption:\033[0m", pred_caption)
   # plot_attention_map(result, attention_plot, test_image)
    # show_attention_heatmap(test_image, attention_plot, result)
    # show_attention_stac(test_image, attention_plot, result)
    speech = gTTS('Predicted Caption : ' + pred_caption, lang = 'en', slow = False)
    speech.save('voice.mp3')
    audio_file = 'voice.mp3'

    display.display(display.Audio(audio_file, rate = None, autoplay = autoplay))

    return test_image

def pred_cap_for_test(image_name, autoplay=False) :
    autoplay = False
    print(image_name)
    print()

    # image_name = "/content/all_data/validation_images/110802332.jpg"

    # rid = image_test.index(image_name)
    # print(rid)

    # cap_test_data = caption_val.copy()
    # rid = 100
    # rid = random.sample(range(0, rndm),2)
    # real_cap_list=[]
    pred_cap_list=[]
    # j=rid
    test_image=image_name
    #print(len(cap_test_data))
    # real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test_data[j] if i not in [0]])
    #print(real_caption)
    # result, attention_plot, pred_test = evaluate(test_image)
    result, pred_test = evaluate(test_image)
    # print(attention_plot.shape)

    # real_caption=filt_text(real_caption)
    pred_caption=' '.join(result).rsplit(' ', 1)[0]
    # real_appn = []
    # real_appn.append(real_caption.split())
    # real_cap_list.append(real_appn)
    pred_cap_list.append(pred_caption.split())

    #all_eval_method(test_image,real_cap_list, pred_cap_list)

    # print("\033[1mReal Caption:\033[0m", real_caption)
    print("\033[1mPrediction Caption:\033[0m", pred_caption)
   # plot_attention_map(result, attention_plot, test_image)
    # show_attention_heatmap(test_image, attention_plot, result)
    # show_attention_stac(test_image, attention_plot, result)
    speech = gTTS('Predicted Caption : ' + pred_caption, lang = 'en', slow = False)
    speech.save('voice.mp3')
    audio_file = 'voice.mp3'

    display.display(display.Audio(audio_file, rate = None, autoplay = autoplay))

    return test_image

image_name =  "/content/all_data/validation_images/100652400.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/1073444492.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/102851549.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/1121053156.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/113032513.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/128912885.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/4196151169.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/6188774172.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/3015487184.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/302740416.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/314779208.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/3212625256.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/379612511.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

"""**==========================================================**"""

image_name =  "/content/all_data/validation_images/3453313865.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/3062582231.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/3669315178.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/4059413955.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/365759754.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/3721404396.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/3834502656.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/4051627887.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/validation_images/434171515.jpg"
test_image = predicted_captiontest(image_name, True)
Image.open(test_image)

"""**----------------Test Images---------------------------**"""

image_name =  "/content/all_data/testing_images/1011572216.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/7162685234.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/5086989679.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/4283472819.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/4439654945.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/2230134548.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/4606522320.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/4376363559.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/6338704.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/1330645772.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/2431723485.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/4967261262.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)

image_name =  "/content/all_data/testing_images/1053116826.jpg"
test_image = pred_cap_for_test(image_name, True)
Image.open(test_image)